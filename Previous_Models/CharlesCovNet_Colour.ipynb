{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/INT2-19/INT2_PyTorch-CrashCourse/blob/main/CharlesCovNet_Colour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCDm0X9sSkoo",
        "outputId": "7b87f749-d6ed-4339-b3ad-1f722b5d3c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/250], Train Loss: 17.5633, Train Acc: 1.37%\n",
            "Epoch [2/250], Train Loss: 6.2609, Train Acc: 2.25%\n",
            "Epoch [3/250], Train Loss: 4.5755, Train Acc: 2.45%\n",
            "Epoch [4/250], Train Loss: 4.5033, Train Acc: 2.75%\n",
            "Epoch [5/250], Train Loss: 4.4257, Train Acc: 3.24%\n",
            "Epoch [6/250], Train Loss: 4.3278, Train Acc: 4.41%\n",
            "Epoch [7/250], Train Loss: 4.2299, Train Acc: 4.90%\n",
            "Epoch [8/250], Train Loss: 4.1600, Train Acc: 6.47%\n",
            "Epoch [9/250], Train Loss: 4.0447, Train Acc: 7.84%\n",
            "Epoch [10/250], Train Loss: 3.9906, Train Acc: 7.84%\n",
            "Epoch [11/250], Train Loss: 3.9296, Train Acc: 8.53%\n",
            "Epoch [12/250], Train Loss: 3.9498, Train Acc: 9.22%\n",
            "Epoch [13/250], Train Loss: 3.8849, Train Acc: 9.80%\n",
            "Epoch [14/250], Train Loss: 3.8413, Train Acc: 9.12%\n",
            "Epoch [15/250], Train Loss: 3.7667, Train Acc: 11.18%\n",
            "Epoch [16/250], Train Loss: 3.7888, Train Acc: 11.67%\n",
            "Epoch [17/250], Train Loss: 3.7390, Train Acc: 13.33%\n",
            "Epoch [18/250], Train Loss: 3.7621, Train Acc: 11.27%\n",
            "Epoch [19/250], Train Loss: 3.6817, Train Acc: 14.31%\n",
            "Epoch [20/250], Train Loss: 3.6810, Train Acc: 13.43%\n",
            "Epoch [21/250], Train Loss: 3.6812, Train Acc: 13.24%\n",
            "Epoch [22/250], Train Loss: 3.6517, Train Acc: 14.51%\n",
            "Epoch [23/250], Train Loss: 3.6410, Train Acc: 13.92%\n",
            "Epoch [24/250], Train Loss: 3.6275, Train Acc: 13.92%\n",
            "Epoch [25/250], Train Loss: 3.5974, Train Acc: 14.80%, Valid Loss: 3.4954, Valid Acc: 14.65%\n",
            "Epoch [26/250], Train Loss: 3.5981, Train Acc: 15.69%\n",
            "Epoch [27/250], Train Loss: 3.5756, Train Acc: 17.25%\n",
            "Epoch [28/250], Train Loss: 3.5933, Train Acc: 15.59%\n",
            "Epoch [29/250], Train Loss: 3.5061, Train Acc: 16.37%\n",
            "Epoch [30/250], Train Loss: 3.4964, Train Acc: 16.86%\n",
            "Epoch [31/250], Train Loss: 3.4143, Train Acc: 17.06%\n",
            "Epoch [32/250], Train Loss: 3.5055, Train Acc: 17.84%\n",
            "Epoch [33/250], Train Loss: 3.4553, Train Acc: 16.37%\n",
            "Epoch [34/250], Train Loss: 3.4501, Train Acc: 18.82%\n",
            "Epoch [35/250], Train Loss: 3.4798, Train Acc: 19.90%\n",
            "Epoch [36/250], Train Loss: 3.4389, Train Acc: 18.73%\n",
            "Epoch [37/250], Train Loss: 3.4074, Train Acc: 20.49%\n",
            "Epoch [38/250], Train Loss: 3.4067, Train Acc: 17.84%\n",
            "Epoch [39/250], Train Loss: 3.3929, Train Acc: 22.65%\n",
            "Epoch [40/250], Train Loss: 3.3203, Train Acc: 22.06%\n",
            "Epoch [41/250], Train Loss: 3.3327, Train Acc: 20.78%\n",
            "Epoch [42/250], Train Loss: 3.3279, Train Acc: 22.06%\n",
            "Epoch [43/250], Train Loss: 3.3102, Train Acc: 21.47%\n",
            "Epoch [44/250], Train Loss: 3.2715, Train Acc: 22.55%\n",
            "Epoch [45/250], Train Loss: 3.2831, Train Acc: 19.90%\n",
            "Epoch [46/250], Train Loss: 3.3004, Train Acc: 21.67%\n",
            "Epoch [47/250], Train Loss: 3.2199, Train Acc: 23.43%\n",
            "Epoch [48/250], Train Loss: 3.3072, Train Acc: 21.76%\n",
            "Epoch [49/250], Train Loss: 3.2063, Train Acc: 24.80%\n",
            "Epoch [50/250], Train Loss: 3.2375, Train Acc: 24.80%, Valid Loss: 3.2846, Valid Acc: 19.78%\n",
            "Epoch [51/250], Train Loss: 3.2129, Train Acc: 26.57%\n",
            "Epoch [52/250], Train Loss: 3.1817, Train Acc: 25.88%\n",
            "Epoch [53/250], Train Loss: 3.2239, Train Acc: 24.02%\n",
            "Epoch [54/250], Train Loss: 3.1714, Train Acc: 27.45%\n",
            "Epoch [55/250], Train Loss: 3.1762, Train Acc: 24.80%\n",
            "Epoch [56/250], Train Loss: 3.2382, Train Acc: 23.04%\n",
            "Epoch [57/250], Train Loss: 3.0998, Train Acc: 29.31%\n",
            "Epoch [58/250], Train Loss: 3.1375, Train Acc: 26.18%\n",
            "Epoch [59/250], Train Loss: 3.1877, Train Acc: 26.27%\n",
            "Epoch [60/250], Train Loss: 3.1413, Train Acc: 27.65%\n",
            "Epoch [61/250], Train Loss: 3.0745, Train Acc: 27.25%\n",
            "Epoch [62/250], Train Loss: 3.0950, Train Acc: 30.98%\n",
            "Epoch [63/250], Train Loss: 3.0925, Train Acc: 30.00%\n",
            "Epoch [64/250], Train Loss: 3.0646, Train Acc: 29.22%\n",
            "Epoch [65/250], Train Loss: 3.0640, Train Acc: 30.29%\n",
            "Epoch [66/250], Train Loss: 2.9608, Train Acc: 32.35%\n",
            "Epoch [67/250], Train Loss: 3.0279, Train Acc: 29.31%\n",
            "Epoch [68/250], Train Loss: 3.0042, Train Acc: 29.51%\n",
            "Epoch [69/250], Train Loss: 3.0096, Train Acc: 30.88%\n",
            "Epoch [70/250], Train Loss: 3.0627, Train Acc: 31.08%\n",
            "Epoch [71/250], Train Loss: 2.9796, Train Acc: 31.86%\n",
            "Epoch [72/250], Train Loss: 2.9917, Train Acc: 30.88%\n",
            "Epoch [73/250], Train Loss: 2.9084, Train Acc: 32.65%\n",
            "Epoch [74/250], Train Loss: 2.9521, Train Acc: 32.06%\n",
            "Epoch [75/250], Train Loss: 2.8933, Train Acc: 35.29%, Valid Loss: 3.1845, Valid Acc: 24.52%\n",
            "Epoch [76/250], Train Loss: 2.8714, Train Acc: 35.20%\n",
            "Epoch [77/250], Train Loss: 2.8551, Train Acc: 37.84%\n",
            "Epoch [78/250], Train Loss: 2.8376, Train Acc: 35.29%\n",
            "Epoch [79/250], Train Loss: 2.7796, Train Acc: 39.22%\n",
            "Epoch [80/250], Train Loss: 2.8576, Train Acc: 35.78%\n",
            "Epoch [81/250], Train Loss: 2.8103, Train Acc: 37.06%\n",
            "Epoch [82/250], Train Loss: 2.7864, Train Acc: 36.47%\n",
            "Epoch [83/250], Train Loss: 2.7688, Train Acc: 36.86%\n",
            "Epoch [84/250], Train Loss: 2.7933, Train Acc: 38.53%\n",
            "Epoch [85/250], Train Loss: 2.7231, Train Acc: 40.20%\n",
            "Epoch [86/250], Train Loss: 2.8049, Train Acc: 36.67%\n",
            "Epoch [87/250], Train Loss: 2.7369, Train Acc: 38.82%\n",
            "Epoch [88/250], Train Loss: 2.7394, Train Acc: 40.20%\n",
            "Epoch [89/250], Train Loss: 2.7306, Train Acc: 38.53%\n",
            "Epoch [90/250], Train Loss: 2.7393, Train Acc: 37.25%\n",
            "Epoch [91/250], Train Loss: 2.7485, Train Acc: 39.02%\n",
            "Epoch [92/250], Train Loss: 2.7580, Train Acc: 40.00%\n",
            "Epoch [93/250], Train Loss: 2.6940, Train Acc: 40.49%\n",
            "Epoch [94/250], Train Loss: 2.6900, Train Acc: 39.71%\n",
            "Epoch [95/250], Train Loss: 2.6608, Train Acc: 42.75%\n",
            "Epoch [96/250], Train Loss: 2.6288, Train Acc: 43.14%\n",
            "Epoch [97/250], Train Loss: 2.6947, Train Acc: 41.86%\n",
            "Epoch [98/250], Train Loss: 2.6940, Train Acc: 41.08%\n",
            "Epoch [99/250], Train Loss: 2.6519, Train Acc: 43.53%\n",
            "Epoch [100/250], Train Loss: 2.6745, Train Acc: 43.73%, Valid Loss: 2.9514, Valid Acc: 29.39%\n",
            "Epoch [101/250], Train Loss: 2.6431, Train Acc: 43.53%\n",
            "Epoch [102/250], Train Loss: 2.6314, Train Acc: 43.14%\n",
            "Epoch [103/250], Train Loss: 2.5694, Train Acc: 46.86%\n",
            "Epoch [104/250], Train Loss: 2.6413, Train Acc: 44.51%\n",
            "Epoch [105/250], Train Loss: 2.5784, Train Acc: 43.63%\n",
            "Epoch [106/250], Train Loss: 2.5563, Train Acc: 46.37%\n",
            "Epoch [107/250], Train Loss: 2.5127, Train Acc: 47.16%\n",
            "Epoch [108/250], Train Loss: 2.4859, Train Acc: 46.27%\n",
            "Epoch [109/250], Train Loss: 2.5474, Train Acc: 46.37%\n",
            "Epoch [110/250], Train Loss: 2.4908, Train Acc: 48.63%\n",
            "Epoch [111/250], Train Loss: 2.4862, Train Acc: 46.27%\n",
            "Epoch [112/250], Train Loss: 2.5514, Train Acc: 46.47%\n",
            "Epoch [113/250], Train Loss: 2.4806, Train Acc: 47.06%\n",
            "Epoch [114/250], Train Loss: 2.4956, Train Acc: 49.41%\n",
            "Epoch [115/250], Train Loss: 2.4721, Train Acc: 48.63%\n",
            "Epoch [116/250], Train Loss: 2.4808, Train Acc: 48.14%\n",
            "Epoch [117/250], Train Loss: 2.5024, Train Acc: 47.16%\n",
            "Epoch [118/250], Train Loss: 2.4362, Train Acc: 48.04%\n",
            "Epoch [119/250], Train Loss: 2.4014, Train Acc: 50.20%\n",
            "Epoch [120/250], Train Loss: 2.3978, Train Acc: 49.90%\n",
            "Epoch [121/250], Train Loss: 2.3941, Train Acc: 52.06%\n",
            "Epoch [122/250], Train Loss: 2.3441, Train Acc: 52.55%\n",
            "Epoch [123/250], Train Loss: 2.4572, Train Acc: 49.71%\n",
            "Epoch [124/250], Train Loss: 2.3822, Train Acc: 52.55%\n",
            "Epoch [125/250], Train Loss: 2.3261, Train Acc: 54.02%, Valid Loss: 2.8119, Valid Acc: 33.27%\n",
            "Epoch [126/250], Train Loss: 2.3636, Train Acc: 53.24%\n",
            "Epoch [127/250], Train Loss: 2.3550, Train Acc: 52.16%\n",
            "Epoch [128/250], Train Loss: 2.3403, Train Acc: 53.14%\n",
            "Epoch [129/250], Train Loss: 2.3741, Train Acc: 51.67%\n",
            "Epoch [130/250], Train Loss: 2.3353, Train Acc: 52.84%\n",
            "Epoch [131/250], Train Loss: 2.2816, Train Acc: 55.10%\n",
            "Epoch [132/250], Train Loss: 2.3053, Train Acc: 56.57%\n",
            "Epoch [133/250], Train Loss: 2.2936, Train Acc: 55.88%\n",
            "Epoch [134/250], Train Loss: 2.3326, Train Acc: 53.33%\n",
            "Epoch [135/250], Train Loss: 2.3066, Train Acc: 53.73%\n",
            "Epoch [136/250], Train Loss: 2.2860, Train Acc: 55.39%\n",
            "Epoch [137/250], Train Loss: 2.2744, Train Acc: 56.47%\n",
            "Epoch [138/250], Train Loss: 2.2663, Train Acc: 56.57%\n",
            "Epoch [139/250], Train Loss: 2.2878, Train Acc: 55.59%\n",
            "Epoch [140/250], Train Loss: 2.2309, Train Acc: 57.35%\n",
            "Epoch [141/250], Train Loss: 2.2775, Train Acc: 54.02%\n",
            "Epoch [142/250], Train Loss: 2.2237, Train Acc: 58.92%\n",
            "Epoch [143/250], Train Loss: 2.2105, Train Acc: 59.02%\n",
            "Epoch [144/250], Train Loss: 2.1850, Train Acc: 58.82%\n",
            "Epoch [145/250], Train Loss: 2.1651, Train Acc: 59.51%\n",
            "Epoch [146/250], Train Loss: 2.1627, Train Acc: 59.90%\n",
            "Epoch [147/250], Train Loss: 2.1579, Train Acc: 60.69%\n",
            "Epoch [148/250], Train Loss: 2.1451, Train Acc: 60.00%\n",
            "Epoch [149/250], Train Loss: 2.1772, Train Acc: 59.41%\n",
            "Epoch [150/250], Train Loss: 2.1708, Train Acc: 59.22%, Valid Loss: 2.7987, Valid Acc: 34.31%\n",
            "Epoch [151/250], Train Loss: 2.1064, Train Acc: 62.06%\n",
            "Epoch [152/250], Train Loss: 2.1546, Train Acc: 60.29%\n",
            "Epoch [153/250], Train Loss: 2.1126, Train Acc: 63.24%\n",
            "Epoch [154/250], Train Loss: 2.0989, Train Acc: 63.82%\n",
            "Epoch [155/250], Train Loss: 2.1113, Train Acc: 62.35%\n",
            "Epoch [156/250], Train Loss: 2.1075, Train Acc: 63.14%\n",
            "Epoch [157/250], Train Loss: 2.1036, Train Acc: 61.76%\n",
            "Epoch [158/250], Train Loss: 2.1873, Train Acc: 58.14%\n",
            "Epoch [159/250], Train Loss: 2.0886, Train Acc: 63.73%\n",
            "Epoch [160/250], Train Loss: 2.1018, Train Acc: 59.90%\n",
            "Epoch [161/250], Train Loss: 2.1168, Train Acc: 61.67%\n",
            "Epoch [162/250], Train Loss: 2.0990, Train Acc: 62.94%\n",
            "Epoch [163/250], Train Loss: 2.0880, Train Acc: 63.24%\n",
            "Epoch [164/250], Train Loss: 2.0811, Train Acc: 60.98%\n",
            "Epoch [165/250], Train Loss: 2.0798, Train Acc: 63.04%\n",
            "Epoch [166/250], Train Loss: 2.0496, Train Acc: 64.22%\n",
            "Epoch [167/250], Train Loss: 2.0372, Train Acc: 66.08%\n",
            "Epoch [168/250], Train Loss: 2.0712, Train Acc: 64.41%\n",
            "Epoch [169/250], Train Loss: 2.0344, Train Acc: 66.96%\n",
            "Epoch [170/250], Train Loss: 2.0312, Train Acc: 64.71%\n",
            "Epoch [171/250], Train Loss: 2.0556, Train Acc: 62.75%\n",
            "Epoch [172/250], Train Loss: 2.0191, Train Acc: 65.98%\n",
            "Epoch [173/250], Train Loss: 2.0444, Train Acc: 65.10%\n",
            "Epoch [174/250], Train Loss: 1.9796, Train Acc: 69.41%\n",
            "Epoch [175/250], Train Loss: 1.9978, Train Acc: 65.88%, Valid Loss: 2.6349, Valid Acc: 39.32%\n",
            "Epoch [176/250], Train Loss: 1.9689, Train Acc: 67.84%\n",
            "Epoch [177/250], Train Loss: 2.0353, Train Acc: 65.10%\n",
            "Epoch [178/250], Train Loss: 1.9918, Train Acc: 68.24%\n",
            "Epoch [179/250], Train Loss: 1.9792, Train Acc: 67.55%\n",
            "Epoch [180/250], Train Loss: 1.9751, Train Acc: 68.73%\n",
            "Epoch [181/250], Train Loss: 1.9704, Train Acc: 67.55%\n",
            "Epoch [182/250], Train Loss: 2.0040, Train Acc: 65.88%\n",
            "Epoch [183/250], Train Loss: 1.9935, Train Acc: 68.33%\n",
            "Epoch [184/250], Train Loss: 1.9323, Train Acc: 70.29%\n",
            "Epoch [185/250], Train Loss: 1.9943, Train Acc: 66.96%\n",
            "Epoch [186/250], Train Loss: 1.9946, Train Acc: 66.47%\n",
            "Epoch [187/250], Train Loss: 1.9813, Train Acc: 67.06%\n",
            "Epoch [188/250], Train Loss: 1.9548, Train Acc: 68.14%\n",
            "Epoch [189/250], Train Loss: 1.9182, Train Acc: 70.20%\n",
            "Epoch [190/250], Train Loss: 1.9224, Train Acc: 70.20%\n",
            "Epoch [191/250], Train Loss: 1.9111, Train Acc: 70.78%\n",
            "Epoch [192/250], Train Loss: 1.9216, Train Acc: 70.20%\n",
            "Epoch [193/250], Train Loss: 1.9211, Train Acc: 68.33%\n",
            "Epoch [194/250], Train Loss: 1.9374, Train Acc: 69.12%\n",
            "Epoch [195/250], Train Loss: 1.9186, Train Acc: 70.00%\n",
            "Epoch [196/250], Train Loss: 1.8914, Train Acc: 72.84%\n",
            "Epoch [197/250], Train Loss: 1.8530, Train Acc: 71.47%\n",
            "Epoch [198/250], Train Loss: 1.9477, Train Acc: 68.04%\n",
            "Epoch [199/250], Train Loss: 1.8633, Train Acc: 72.35%\n",
            "Epoch [200/250], Train Loss: 1.9146, Train Acc: 71.96%, Valid Loss: 2.5985, Valid Acc: 40.54%\n",
            "Epoch [201/250], Train Loss: 1.8447, Train Acc: 74.51%\n",
            "Epoch [202/250], Train Loss: 1.8885, Train Acc: 71.27%\n",
            "Epoch [203/250], Train Loss: 1.8404, Train Acc: 74.41%\n",
            "Epoch [204/250], Train Loss: 1.8687, Train Acc: 73.24%\n",
            "Epoch [205/250], Train Loss: 1.8213, Train Acc: 75.59%\n",
            "Epoch [206/250], Train Loss: 1.8073, Train Acc: 74.51%\n",
            "Epoch [207/250], Train Loss: 1.8540, Train Acc: 73.14%\n",
            "Epoch [208/250], Train Loss: 1.8379, Train Acc: 72.75%\n",
            "Epoch [209/250], Train Loss: 1.7922, Train Acc: 74.61%\n",
            "Epoch [210/250], Train Loss: 1.7800, Train Acc: 75.88%\n",
            "Epoch [211/250], Train Loss: 1.7751, Train Acc: 75.78%\n",
            "Epoch [212/250], Train Loss: 1.7928, Train Acc: 74.90%\n",
            "Epoch [213/250], Train Loss: 1.7502, Train Acc: 75.20%\n",
            "Epoch [214/250], Train Loss: 1.7658, Train Acc: 77.06%\n",
            "Epoch [215/250], Train Loss: 1.8210, Train Acc: 74.22%\n",
            "Epoch [216/250], Train Loss: 1.8349, Train Acc: 73.63%\n",
            "Epoch [217/250], Train Loss: 1.8126, Train Acc: 74.80%\n",
            "Epoch [218/250], Train Loss: 1.7995, Train Acc: 74.90%\n",
            "Epoch [219/250], Train Loss: 1.8034, Train Acc: 75.20%\n",
            "Epoch [220/250], Train Loss: 1.7792, Train Acc: 75.29%\n",
            "Epoch [221/250], Train Loss: 1.7458, Train Acc: 75.10%\n",
            "Epoch [222/250], Train Loss: 1.7906, Train Acc: 75.10%\n",
            "Epoch [223/250], Train Loss: 1.7264, Train Acc: 78.63%\n",
            "Epoch [224/250], Train Loss: 1.7543, Train Acc: 77.65%\n",
            "Epoch [225/250], Train Loss: 1.7740, Train Acc: 74.71%, Valid Loss: 2.5810, Valid Acc: 41.52%\n",
            "Epoch [226/250], Train Loss: 1.7460, Train Acc: 75.88%\n",
            "Epoch [227/250], Train Loss: 1.7674, Train Acc: 76.76%\n",
            "Epoch [228/250], Train Loss: 1.7241, Train Acc: 78.14%\n",
            "Epoch [229/250], Train Loss: 1.7397, Train Acc: 76.37%\n",
            "Epoch [230/250], Train Loss: 1.7413, Train Acc: 78.63%\n",
            "Epoch [231/250], Train Loss: 1.7186, Train Acc: 78.24%\n",
            "Epoch [232/250], Train Loss: 1.7581, Train Acc: 76.67%\n",
            "Epoch [233/250], Train Loss: 1.6913, Train Acc: 78.14%\n",
            "Epoch [234/250], Train Loss: 1.7005, Train Acc: 79.90%\n",
            "Epoch [235/250], Train Loss: 1.7618, Train Acc: 75.69%\n",
            "Epoch [236/250], Train Loss: 1.7541, Train Acc: 76.96%\n",
            "Epoch [237/250], Train Loss: 1.6955, Train Acc: 78.33%\n",
            "Epoch [238/250], Train Loss: 1.6602, Train Acc: 79.90%\n",
            "Epoch [239/250], Train Loss: 1.6929, Train Acc: 79.90%\n",
            "Epoch [240/250], Train Loss: 1.7185, Train Acc: 77.75%\n",
            "Epoch [241/250], Train Loss: 1.6651, Train Acc: 79.02%\n",
            "Epoch [242/250], Train Loss: 1.7020, Train Acc: 78.53%\n",
            "Epoch [243/250], Train Loss: 1.7051, Train Acc: 77.65%\n",
            "Epoch [244/250], Train Loss: 1.6469, Train Acc: 79.71%\n",
            "Epoch [245/250], Train Loss: 1.6866, Train Acc: 79.41%\n",
            "Epoch [246/250], Train Loss: 1.6490, Train Acc: 80.59%\n",
            "Epoch [247/250], Train Loss: 1.6800, Train Acc: 79.80%\n",
            "Epoch [248/250], Train Loss: 1.6627, Train Acc: 78.24%\n",
            "Epoch [249/250], Train Loss: 1.6725, Train Acc: 79.71%\n",
            "Epoch [250/250], Train Loss: 1.7064, Train Acc: 77.55%, Valid Loss: 2.5153, Valid Acc: 42.71%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 250\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Convolutional neural network\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=102):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(25088, 2048)\n",
        "        self.fc2 = nn.Linear(2048, num_classes)\n",
        "        self.drop = nn.Dropout(0.4)\n",
        "        # Batch normalisation used on convolution layers, dropout 20% used on fully connected linear layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.pool(out)\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.pool(out)\n",
        "        out = F.relu(self.bn3(self.conv3(out)))\n",
        "        out = self.pool(out)\n",
        "        out = F.relu(self.bn4(self.conv4(out)))\n",
        "        out = self.pool(out)\n",
        "        out = F.relu(self.bn5(self.conv5(out)))\n",
        "        out = self.pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.drop(self.fc1(out)))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Create model and push to device\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "# Get loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define transforms with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomVerticalFlip(p=0.05),\n",
        "    transforms.RandomHorizontalFlip(p=0.1),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.ColorJitter(contrast=0.25, saturation=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Flower102: Unknown number and various sized color images in 102 classes, with 40 to 258 images per class\n",
        "train_dataset = torchvision.datasets.Flowers102(root='./data', split='train',\n",
        "                                                download=True, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.Flowers102(root='./data', split='test',\n",
        "                                               download=True, transform=test_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device, epoch, smoothing=0.1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Apply label smoothing\n",
        "        num_classes = model.fc2.out_features\n",
        "        one_hot_labels = torch.zeros(labels.size(0), num_classes).to(device)\n",
        "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
        "        one_hot_labels = one_hot_labels * (1 - smoothing) + smoothing / num_classes\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, one_hot_labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    loss = train_loss / len(train_loader)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def test(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    loss = test_loss / len(test_loader)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "# Train the model\n",
        "train_losses, train_accs, valid_losses, valid_accs = [], [], [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    if ((epoch + 1) % 25 == 0):\n",
        "      valid_loss, valid_acc = test(model, test_loader, criterion, device)\n",
        "      valid_losses.append(valid_loss)   \n",
        "      valid_accs.append(valid_acc)\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.2f}%')\n",
        "    else:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Did not save model, so load model\n",
        "loaded_model = ConvNet()\n",
        "loaded_model.load_state_dict(torch.load(PATH)) # it takes the loaded dictionary, not the path file itself\n",
        "# Push to device\n",
        "loaded_model.to(device)\n",
        "loaded_model.eval()\n",
        "\n",
        "# Evaluation, no gradient required\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_correct2 = 0\n",
        "    n_samples = len(test_loader.dataset)\n",
        "\n",
        "    for images, labels in test_loader:  # Iterate over test loader\n",
        "        images = images.to(device)      # Push to GPU device\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)    # Compare outputs with lables\n",
        "        n_correct += (predicted == labels).sum().item() # Number of correct\n",
        "\n",
        "        outputs2 = loaded_model(images)       # Same put loaded model\n",
        "        _, predicted2 = torch.max(outputs2, 1)\n",
        "        n_correct2 += (predicted2 == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the model: {acc} %')\n",
        "\n",
        "    acc = 100.0 * n_correct2 / n_samples\n",
        "    print(f'Accuracy of the loaded model: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP8x83M5XMJH",
        "outputId": "13ef1523-c210-4baa-e68e-da3194055583"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 42.706131078224104 %\n",
            "Accuracy of the loaded model: 42.706131078224104 %\n"
          ]
        }
      ]
    }
  ]
}