{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/INT2-19/INT2_PyTorch-CrashCourse/blob/main/FinalCovNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCDm0X9sSkoo",
        "outputId": "f9fb303b-7eee-4732-aca2-440688c80747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Train Loss: 18.0798, Train Acc: 0.39%\n",
            "Epoch [2/500], Train Loss: 6.3761, Train Acc: 1.27%\n",
            "Epoch [3/500], Train Loss: 4.7117, Train Acc: 2.06%\n",
            "Epoch [4/500], Train Loss: 4.5995, Train Acc: 1.37%\n",
            "Epoch [5/500], Train Loss: 4.5413, Train Acc: 1.57%\n",
            "Epoch [6/500], Train Loss: 4.5071, Train Acc: 2.65%\n",
            "Epoch [7/500], Train Loss: 4.4473, Train Acc: 3.04%\n",
            "Epoch [8/500], Train Loss: 4.4163, Train Acc: 2.45%\n",
            "Epoch [9/500], Train Loss: 4.4162, Train Acc: 3.43%\n",
            "Epoch [10/500], Train Loss: 4.3604, Train Acc: 4.02%\n",
            "Epoch [11/500], Train Loss: 4.3440, Train Acc: 4.12%\n",
            "Epoch [12/500], Train Loss: 4.3082, Train Acc: 5.00%\n",
            "Epoch [13/500], Train Loss: 4.2888, Train Acc: 3.33%\n",
            "Epoch [14/500], Train Loss: 4.1880, Train Acc: 5.69%\n",
            "Epoch [15/500], Train Loss: 4.1748, Train Acc: 4.80%\n",
            "Epoch [16/500], Train Loss: 4.1120, Train Acc: 5.49%\n",
            "Epoch [17/500], Train Loss: 4.0735, Train Acc: 5.98%\n",
            "Epoch [18/500], Train Loss: 4.0306, Train Acc: 8.04%\n",
            "Epoch [19/500], Train Loss: 4.0061, Train Acc: 7.75%\n",
            "Epoch [20/500], Train Loss: 3.9987, Train Acc: 8.43%\n",
            "Epoch [21/500], Train Loss: 3.9596, Train Acc: 9.02%\n",
            "Epoch [22/500], Train Loss: 3.9192, Train Acc: 9.02%\n",
            "Epoch [23/500], Train Loss: 3.8734, Train Acc: 11.57%\n",
            "Epoch [24/500], Train Loss: 3.8560, Train Acc: 9.90%\n",
            "Epoch [25/500], Train Loss: 3.8329, Train Acc: 10.39%\n",
            "Epoch [26/500], Train Loss: 3.8139, Train Acc: 10.00%\n",
            "Epoch [27/500], Train Loss: 3.8556, Train Acc: 10.59%\n",
            "Epoch [28/500], Train Loss: 3.7842, Train Acc: 11.37%\n",
            "Epoch [29/500], Train Loss: 3.7781, Train Acc: 11.57%\n",
            "Epoch [30/500], Train Loss: 3.7139, Train Acc: 12.55%\n",
            "Epoch [31/500], Train Loss: 3.7185, Train Acc: 14.02%\n",
            "Epoch [32/500], Train Loss: 3.6662, Train Acc: 12.45%\n",
            "Epoch [33/500], Train Loss: 3.6866, Train Acc: 14.31%\n",
            "Epoch [34/500], Train Loss: 3.6558, Train Acc: 13.43%\n",
            "Epoch [35/500], Train Loss: 3.6639, Train Acc: 14.31%\n",
            "Epoch [36/500], Train Loss: 3.6630, Train Acc: 15.98%\n",
            "Epoch [37/500], Train Loss: 3.6314, Train Acc: 14.02%\n",
            "Epoch [38/500], Train Loss: 3.6176, Train Acc: 15.39%\n",
            "Epoch [39/500], Train Loss: 3.5812, Train Acc: 16.18%\n",
            "Epoch [40/500], Train Loss: 3.5361, Train Acc: 17.65%\n",
            "Epoch [41/500], Train Loss: 3.5378, Train Acc: 16.96%\n",
            "Epoch [42/500], Train Loss: 3.5081, Train Acc: 16.67%\n",
            "Epoch [43/500], Train Loss: 3.5385, Train Acc: 18.14%\n",
            "Epoch [44/500], Train Loss: 3.4219, Train Acc: 19.71%\n",
            "Epoch [45/500], Train Loss: 3.4649, Train Acc: 18.43%\n",
            "Epoch [46/500], Train Loss: 3.4668, Train Acc: 18.14%\n",
            "Epoch [47/500], Train Loss: 3.4076, Train Acc: 20.59%\n",
            "Epoch [48/500], Train Loss: 3.4409, Train Acc: 18.92%\n",
            "Epoch [49/500], Train Loss: 3.4201, Train Acc: 19.41%\n",
            "Epoch [50/500], Train Loss: 3.3887, Train Acc: 19.61%\n",
            "Epoch [51/500], Train Loss: 3.4709, Train Acc: 18.73%\n",
            "Epoch [52/500], Train Loss: 3.4328, Train Acc: 20.69%\n",
            "Epoch [53/500], Train Loss: 3.3632, Train Acc: 21.76%\n",
            "Epoch [54/500], Train Loss: 3.3423, Train Acc: 20.39%\n",
            "Epoch [55/500], Train Loss: 3.3042, Train Acc: 22.45%\n",
            "Epoch [56/500], Train Loss: 3.3760, Train Acc: 21.86%\n",
            "Epoch [57/500], Train Loss: 3.2569, Train Acc: 24.12%\n",
            "Epoch [58/500], Train Loss: 3.2850, Train Acc: 23.04%\n",
            "Epoch [59/500], Train Loss: 3.2539, Train Acc: 24.51%\n",
            "Epoch [60/500], Train Loss: 3.2477, Train Acc: 22.65%\n",
            "Epoch [61/500], Train Loss: 3.2300, Train Acc: 24.22%\n",
            "Epoch [62/500], Train Loss: 3.2240, Train Acc: 25.49%\n",
            "Epoch [63/500], Train Loss: 3.2350, Train Acc: 22.65%\n",
            "Epoch [64/500], Train Loss: 3.1996, Train Acc: 24.61%\n",
            "Epoch [65/500], Train Loss: 3.2306, Train Acc: 25.00%\n",
            "Epoch [66/500], Train Loss: 3.2295, Train Acc: 24.90%\n",
            "Epoch [67/500], Train Loss: 3.1649, Train Acc: 25.20%\n",
            "Epoch [68/500], Train Loss: 3.1907, Train Acc: 24.90%\n",
            "Epoch [69/500], Train Loss: 3.1839, Train Acc: 25.98%\n",
            "Epoch [70/500], Train Loss: 3.1386, Train Acc: 26.86%\n",
            "Epoch [71/500], Train Loss: 3.1661, Train Acc: 27.75%\n",
            "Epoch [72/500], Train Loss: 3.0942, Train Acc: 27.25%\n",
            "Epoch [73/500], Train Loss: 3.1201, Train Acc: 27.16%\n",
            "Epoch [74/500], Train Loss: 3.1475, Train Acc: 26.37%\n",
            "Epoch [75/500], Train Loss: 3.0454, Train Acc: 28.73%\n",
            "Epoch [76/500], Train Loss: 3.0145, Train Acc: 31.57%\n",
            "Epoch [77/500], Train Loss: 3.1146, Train Acc: 27.16%\n",
            "Epoch [78/500], Train Loss: 3.0656, Train Acc: 31.27%\n",
            "Epoch [79/500], Train Loss: 2.9937, Train Acc: 31.96%\n",
            "Epoch [80/500], Train Loss: 3.0037, Train Acc: 30.98%\n",
            "Epoch [81/500], Train Loss: 2.9721, Train Acc: 31.57%\n",
            "Epoch [82/500], Train Loss: 3.0887, Train Acc: 28.24%\n",
            "Epoch [83/500], Train Loss: 2.9832, Train Acc: 32.35%\n",
            "Epoch [84/500], Train Loss: 3.0033, Train Acc: 31.86%\n",
            "Epoch [85/500], Train Loss: 2.9530, Train Acc: 32.75%\n",
            "Epoch [86/500], Train Loss: 3.0067, Train Acc: 32.06%\n",
            "Epoch [87/500], Train Loss: 2.9281, Train Acc: 33.53%\n",
            "Epoch [88/500], Train Loss: 2.8787, Train Acc: 35.20%\n",
            "Epoch [89/500], Train Loss: 2.9215, Train Acc: 34.12%\n",
            "Epoch [90/500], Train Loss: 2.8427, Train Acc: 37.06%\n",
            "Epoch [91/500], Train Loss: 2.8650, Train Acc: 35.78%\n",
            "Epoch [92/500], Train Loss: 2.9347, Train Acc: 35.10%\n",
            "Epoch [93/500], Train Loss: 2.8866, Train Acc: 36.76%\n",
            "Epoch [94/500], Train Loss: 2.8402, Train Acc: 36.37%\n",
            "Epoch [95/500], Train Loss: 2.8274, Train Acc: 37.55%\n",
            "Epoch [96/500], Train Loss: 2.8066, Train Acc: 37.84%\n",
            "Epoch [97/500], Train Loss: 2.7794, Train Acc: 40.00%\n",
            "Epoch [98/500], Train Loss: 2.7463, Train Acc: 41.08%\n",
            "Epoch [99/500], Train Loss: 2.7948, Train Acc: 37.75%\n",
            "Epoch [100/500], Train Loss: 2.7632, Train Acc: 38.43%\n",
            "Epoch [101/500], Train Loss: 2.7153, Train Acc: 40.20%\n",
            "Epoch [102/500], Train Loss: 2.7666, Train Acc: 40.10%\n",
            "Epoch [103/500], Train Loss: 2.7638, Train Acc: 40.29%\n",
            "Epoch [104/500], Train Loss: 2.7160, Train Acc: 40.88%\n",
            "Epoch [105/500], Train Loss: 2.6891, Train Acc: 42.16%\n",
            "Epoch [106/500], Train Loss: 2.6851, Train Acc: 40.98%\n",
            "Epoch [107/500], Train Loss: 2.6797, Train Acc: 41.96%\n",
            "Epoch [108/500], Train Loss: 2.6832, Train Acc: 42.94%\n",
            "Epoch [109/500], Train Loss: 2.7235, Train Acc: 41.96%\n",
            "Epoch [110/500], Train Loss: 2.6338, Train Acc: 44.41%\n",
            "Epoch [111/500], Train Loss: 2.6533, Train Acc: 42.55%\n",
            "Epoch [112/500], Train Loss: 2.6387, Train Acc: 42.84%\n",
            "Epoch [113/500], Train Loss: 2.6057, Train Acc: 44.61%\n",
            "Epoch [114/500], Train Loss: 2.6426, Train Acc: 44.41%\n",
            "Epoch [115/500], Train Loss: 2.6340, Train Acc: 40.98%\n",
            "Epoch [116/500], Train Loss: 2.5703, Train Acc: 45.69%\n",
            "Epoch [117/500], Train Loss: 2.5856, Train Acc: 46.08%\n",
            "Epoch [118/500], Train Loss: 2.6189, Train Acc: 43.92%\n",
            "Epoch [119/500], Train Loss: 2.6276, Train Acc: 44.02%\n",
            "Epoch [120/500], Train Loss: 2.5778, Train Acc: 44.12%\n",
            "Epoch [121/500], Train Loss: 2.5490, Train Acc: 46.27%\n",
            "Epoch [122/500], Train Loss: 2.5050, Train Acc: 47.75%\n",
            "Epoch [123/500], Train Loss: 2.5235, Train Acc: 45.20%\n",
            "Epoch [124/500], Train Loss: 2.4558, Train Acc: 49.80%\n",
            "Epoch [125/500], Train Loss: 2.5788, Train Acc: 46.08%\n",
            "Epoch [126/500], Train Loss: 2.5785, Train Acc: 45.98%\n",
            "Epoch [127/500], Train Loss: 2.5123, Train Acc: 47.45%\n",
            "Epoch [128/500], Train Loss: 2.5288, Train Acc: 47.84%\n",
            "Epoch [129/500], Train Loss: 2.4618, Train Acc: 49.71%\n",
            "Epoch [130/500], Train Loss: 2.5161, Train Acc: 48.14%\n",
            "Epoch [131/500], Train Loss: 2.4825, Train Acc: 50.29%\n",
            "Epoch [132/500], Train Loss: 2.4673, Train Acc: 50.00%\n",
            "Epoch [133/500], Train Loss: 2.4431, Train Acc: 51.67%\n",
            "Epoch [134/500], Train Loss: 2.4745, Train Acc: 49.31%\n",
            "Epoch [135/500], Train Loss: 2.5053, Train Acc: 47.45%\n",
            "Epoch [136/500], Train Loss: 2.4109, Train Acc: 52.94%\n",
            "Epoch [137/500], Train Loss: 2.4219, Train Acc: 50.78%\n",
            "Epoch [138/500], Train Loss: 2.3785, Train Acc: 52.55%\n",
            "Epoch [139/500], Train Loss: 2.4410, Train Acc: 50.00%\n",
            "Epoch [140/500], Train Loss: 2.4058, Train Acc: 52.55%\n",
            "Epoch [141/500], Train Loss: 2.4019, Train Acc: 53.33%\n",
            "Epoch [142/500], Train Loss: 2.3977, Train Acc: 50.98%\n",
            "Epoch [143/500], Train Loss: 2.3452, Train Acc: 55.78%\n",
            "Epoch [144/500], Train Loss: 2.3432, Train Acc: 55.29%\n",
            "Epoch [145/500], Train Loss: 2.2959, Train Acc: 54.80%\n",
            "Epoch [146/500], Train Loss: 2.3866, Train Acc: 53.63%\n",
            "Epoch [147/500], Train Loss: 2.3501, Train Acc: 53.33%\n",
            "Epoch [148/500], Train Loss: 2.3645, Train Acc: 53.82%\n",
            "Epoch [149/500], Train Loss: 2.2547, Train Acc: 58.53%\n",
            "Epoch [150/500], Train Loss: 2.3134, Train Acc: 53.24%\n",
            "Epoch [151/500], Train Loss: 2.2664, Train Acc: 55.49%\n",
            "Epoch [152/500], Train Loss: 2.3443, Train Acc: 54.61%\n",
            "Epoch [153/500], Train Loss: 2.3418, Train Acc: 54.71%\n",
            "Epoch [154/500], Train Loss: 2.2967, Train Acc: 56.18%\n",
            "Epoch [155/500], Train Loss: 2.2441, Train Acc: 58.24%\n",
            "Epoch [156/500], Train Loss: 2.2562, Train Acc: 56.37%\n",
            "Epoch [157/500], Train Loss: 2.2571, Train Acc: 57.16%\n",
            "Epoch [158/500], Train Loss: 2.2460, Train Acc: 58.14%\n",
            "Epoch [159/500], Train Loss: 2.2518, Train Acc: 57.35%\n",
            "Epoch [160/500], Train Loss: 2.2373, Train Acc: 59.41%\n",
            "Epoch [161/500], Train Loss: 2.2563, Train Acc: 56.37%\n",
            "Epoch [162/500], Train Loss: 2.2164, Train Acc: 59.80%\n",
            "Epoch [163/500], Train Loss: 2.3058, Train Acc: 53.63%\n",
            "Epoch [164/500], Train Loss: 2.2588, Train Acc: 58.04%\n",
            "Epoch [165/500], Train Loss: 2.2075, Train Acc: 59.31%\n",
            "Epoch [166/500], Train Loss: 2.1845, Train Acc: 60.00%\n",
            "Epoch [167/500], Train Loss: 2.1469, Train Acc: 61.47%\n",
            "Epoch [168/500], Train Loss: 2.2034, Train Acc: 58.43%\n",
            "Epoch [169/500], Train Loss: 2.1775, Train Acc: 59.51%\n",
            "Epoch [170/500], Train Loss: 2.1352, Train Acc: 62.25%\n",
            "Epoch [171/500], Train Loss: 2.1490, Train Acc: 60.69%\n",
            "Epoch [172/500], Train Loss: 2.1555, Train Acc: 62.65%\n",
            "Epoch [173/500], Train Loss: 2.1290, Train Acc: 62.94%\n",
            "Epoch [174/500], Train Loss: 2.1755, Train Acc: 60.10%\n",
            "Epoch [175/500], Train Loss: 2.1724, Train Acc: 59.80%\n",
            "Epoch [176/500], Train Loss: 2.0785, Train Acc: 63.04%\n",
            "Epoch [177/500], Train Loss: 2.1846, Train Acc: 58.92%\n",
            "Epoch [178/500], Train Loss: 2.1225, Train Acc: 62.16%\n",
            "Epoch [179/500], Train Loss: 2.1093, Train Acc: 64.41%\n",
            "Epoch [180/500], Train Loss: 2.0866, Train Acc: 64.22%\n",
            "Epoch [181/500], Train Loss: 2.1661, Train Acc: 62.45%\n",
            "Epoch [182/500], Train Loss: 2.1246, Train Acc: 62.55%\n",
            "Epoch [183/500], Train Loss: 2.0994, Train Acc: 61.57%\n",
            "Epoch [184/500], Train Loss: 2.0949, Train Acc: 62.25%\n",
            "Epoch [185/500], Train Loss: 2.0492, Train Acc: 65.59%\n",
            "Epoch [186/500], Train Loss: 2.0974, Train Acc: 62.84%\n",
            "Epoch [187/500], Train Loss: 2.0412, Train Acc: 63.63%\n",
            "Epoch [188/500], Train Loss: 2.0555, Train Acc: 64.02%\n",
            "Epoch [189/500], Train Loss: 2.0528, Train Acc: 65.59%\n",
            "Epoch [190/500], Train Loss: 2.0835, Train Acc: 64.61%\n",
            "Epoch [191/500], Train Loss: 2.0303, Train Acc: 66.96%\n",
            "Epoch [192/500], Train Loss: 2.1016, Train Acc: 64.80%\n",
            "Epoch [193/500], Train Loss: 1.9901, Train Acc: 68.53%\n",
            "Epoch [194/500], Train Loss: 1.9761, Train Acc: 68.53%\n",
            "Epoch [195/500], Train Loss: 2.0377, Train Acc: 66.57%\n",
            "Epoch [196/500], Train Loss: 1.9977, Train Acc: 67.65%\n",
            "Epoch [197/500], Train Loss: 2.0133, Train Acc: 66.08%\n",
            "Epoch [198/500], Train Loss: 2.0402, Train Acc: 67.45%\n",
            "Epoch [199/500], Train Loss: 2.0326, Train Acc: 64.90%\n",
            "Epoch [200/500], Train Loss: 2.0244, Train Acc: 64.71%\n",
            "Epoch [201/500], Train Loss: 2.0010, Train Acc: 67.75%\n",
            "Epoch [202/500], Train Loss: 1.9841, Train Acc: 67.75%\n",
            "Epoch [203/500], Train Loss: 1.9699, Train Acc: 67.75%\n",
            "Epoch [204/500], Train Loss: 1.9799, Train Acc: 68.63%\n",
            "Epoch [205/500], Train Loss: 1.9911, Train Acc: 67.06%\n",
            "Epoch [206/500], Train Loss: 2.0296, Train Acc: 66.76%\n",
            "Epoch [207/500], Train Loss: 2.0003, Train Acc: 66.67%\n",
            "Epoch [208/500], Train Loss: 1.9645, Train Acc: 69.51%\n",
            "Epoch [209/500], Train Loss: 1.9634, Train Acc: 69.12%\n",
            "Epoch [210/500], Train Loss: 1.9060, Train Acc: 70.10%\n",
            "Epoch [211/500], Train Loss: 1.9458, Train Acc: 71.08%\n",
            "Epoch [212/500], Train Loss: 1.9805, Train Acc: 69.12%\n",
            "Epoch [213/500], Train Loss: 1.9547, Train Acc: 70.39%\n",
            "Epoch [214/500], Train Loss: 1.9154, Train Acc: 70.39%\n",
            "Epoch [215/500], Train Loss: 1.8712, Train Acc: 72.25%\n",
            "Epoch [216/500], Train Loss: 1.9364, Train Acc: 69.71%\n",
            "Epoch [217/500], Train Loss: 1.9473, Train Acc: 69.80%\n",
            "Epoch [218/500], Train Loss: 1.9357, Train Acc: 70.20%\n",
            "Epoch [219/500], Train Loss: 1.9170, Train Acc: 70.69%\n",
            "Epoch [220/500], Train Loss: 1.8760, Train Acc: 70.29%\n",
            "Epoch [221/500], Train Loss: 1.8898, Train Acc: 72.84%\n",
            "Epoch [222/500], Train Loss: 1.9039, Train Acc: 70.29%\n",
            "Epoch [223/500], Train Loss: 1.9096, Train Acc: 70.78%\n",
            "Epoch [224/500], Train Loss: 1.8839, Train Acc: 71.67%\n",
            "Epoch [225/500], Train Loss: 1.8952, Train Acc: 73.14%\n",
            "Epoch [226/500], Train Loss: 1.8570, Train Acc: 72.84%\n",
            "Epoch [227/500], Train Loss: 1.8823, Train Acc: 70.98%\n",
            "Epoch [228/500], Train Loss: 1.8415, Train Acc: 74.12%\n",
            "Epoch [229/500], Train Loss: 1.8974, Train Acc: 70.98%\n",
            "Epoch [230/500], Train Loss: 1.9033, Train Acc: 69.90%\n",
            "Epoch [231/500], Train Loss: 1.8397, Train Acc: 75.69%\n",
            "Epoch [232/500], Train Loss: 1.8808, Train Acc: 71.96%\n",
            "Epoch [233/500], Train Loss: 1.8222, Train Acc: 74.22%\n",
            "Epoch [234/500], Train Loss: 1.7890, Train Acc: 75.29%\n",
            "Epoch [235/500], Train Loss: 1.8342, Train Acc: 73.63%\n",
            "Epoch [236/500], Train Loss: 1.8335, Train Acc: 74.61%\n",
            "Epoch [237/500], Train Loss: 1.8633, Train Acc: 72.65%\n",
            "Epoch [238/500], Train Loss: 1.8378, Train Acc: 74.22%\n",
            "Epoch [239/500], Train Loss: 1.8726, Train Acc: 70.00%\n",
            "Epoch [240/500], Train Loss: 1.8729, Train Acc: 71.76%\n",
            "Epoch [241/500], Train Loss: 1.8880, Train Acc: 72.25%\n",
            "Epoch [242/500], Train Loss: 1.8390, Train Acc: 74.12%\n",
            "Epoch [243/500], Train Loss: 1.8539, Train Acc: 73.33%\n",
            "Epoch [244/500], Train Loss: 1.8669, Train Acc: 73.24%\n",
            "Epoch [245/500], Train Loss: 1.8783, Train Acc: 73.14%\n",
            "Epoch [246/500], Train Loss: 1.8105, Train Acc: 74.51%\n",
            "Epoch [247/500], Train Loss: 1.8242, Train Acc: 73.43%\n",
            "Epoch [248/500], Train Loss: 1.7991, Train Acc: 77.06%\n",
            "Epoch [249/500], Train Loss: 1.8164, Train Acc: 73.43%\n",
            "Epoch [250/500], Train Loss: 1.7818, Train Acc: 74.41%\n",
            "Epoch [251/500], Train Loss: 1.7989, Train Acc: 75.20%\n",
            "Epoch [252/500], Train Loss: 1.7784, Train Acc: 78.53%\n",
            "Epoch [253/500], Train Loss: 1.8060, Train Acc: 75.00%\n",
            "Epoch [254/500], Train Loss: 1.8158, Train Acc: 74.80%\n",
            "Epoch [255/500], Train Loss: 1.8565, Train Acc: 72.16%\n",
            "Epoch [256/500], Train Loss: 1.7517, Train Acc: 78.53%\n",
            "Epoch [257/500], Train Loss: 1.7961, Train Acc: 75.49%\n",
            "Epoch [258/500], Train Loss: 1.7796, Train Acc: 77.25%\n",
            "Epoch [259/500], Train Loss: 1.7451, Train Acc: 78.04%\n",
            "Epoch [260/500], Train Loss: 1.7667, Train Acc: 77.35%\n",
            "Epoch [261/500], Train Loss: 1.8025, Train Acc: 74.31%\n",
            "Epoch [262/500], Train Loss: 1.7808, Train Acc: 75.39%\n",
            "Epoch [263/500], Train Loss: 1.7351, Train Acc: 78.82%\n",
            "Epoch [264/500], Train Loss: 1.7531, Train Acc: 77.55%\n",
            "Epoch [265/500], Train Loss: 1.7366, Train Acc: 78.33%\n",
            "Epoch [266/500], Train Loss: 1.7566, Train Acc: 77.35%\n",
            "Epoch [267/500], Train Loss: 1.7793, Train Acc: 76.08%\n",
            "Epoch [268/500], Train Loss: 1.7618, Train Acc: 77.16%\n",
            "Epoch [269/500], Train Loss: 1.7413, Train Acc: 77.35%\n",
            "Epoch [270/500], Train Loss: 1.7247, Train Acc: 78.43%\n",
            "Epoch [271/500], Train Loss: 1.7034, Train Acc: 79.71%\n",
            "Epoch [272/500], Train Loss: 1.7161, Train Acc: 78.53%\n",
            "Epoch [273/500], Train Loss: 1.7406, Train Acc: 79.41%\n",
            "Epoch [274/500], Train Loss: 1.6901, Train Acc: 79.80%\n",
            "Epoch [275/500], Train Loss: 1.7361, Train Acc: 77.55%, Valid Loss: 2.4695, Valid Acc: 43.36%\n",
            "Epoch [276/500], Train Loss: 1.6970, Train Acc: 78.63%\n",
            "Epoch [277/500], Train Loss: 1.7047, Train Acc: 79.51%\n",
            "Epoch [278/500], Train Loss: 1.6918, Train Acc: 80.29%\n",
            "Epoch [279/500], Train Loss: 1.7007, Train Acc: 78.53%\n",
            "Epoch [280/500], Train Loss: 1.7325, Train Acc: 78.33%\n",
            "Epoch [281/500], Train Loss: 1.7215, Train Acc: 78.82%\n",
            "Epoch [282/500], Train Loss: 1.7000, Train Acc: 78.92%\n",
            "Epoch [283/500], Train Loss: 1.6857, Train Acc: 79.61%\n",
            "Epoch [284/500], Train Loss: 1.6768, Train Acc: 78.92%\n",
            "Epoch [285/500], Train Loss: 1.7241, Train Acc: 78.63%\n",
            "Epoch [286/500], Train Loss: 1.6704, Train Acc: 79.90%\n",
            "Epoch [287/500], Train Loss: 1.6510, Train Acc: 82.16%\n",
            "Epoch [288/500], Train Loss: 1.6872, Train Acc: 78.14%\n",
            "Epoch [289/500], Train Loss: 1.6713, Train Acc: 79.71%\n",
            "Epoch [290/500], Train Loss: 1.6695, Train Acc: 78.73%\n",
            "Epoch [291/500], Train Loss: 1.6658, Train Acc: 79.80%\n",
            "Epoch [292/500], Train Loss: 1.6777, Train Acc: 79.22%\n",
            "Epoch [293/500], Train Loss: 1.6843, Train Acc: 78.53%\n",
            "Epoch [294/500], Train Loss: 1.6717, Train Acc: 80.29%\n",
            "Epoch [295/500], Train Loss: 1.6479, Train Acc: 80.20%\n",
            "Epoch [296/500], Train Loss: 1.6276, Train Acc: 82.45%\n",
            "Epoch [297/500], Train Loss: 1.6710, Train Acc: 79.80%\n",
            "Epoch [298/500], Train Loss: 1.6710, Train Acc: 80.29%\n",
            "Epoch [299/500], Train Loss: 1.6495, Train Acc: 80.69%\n",
            "Epoch [300/500], Train Loss: 1.6257, Train Acc: 82.35%, Valid Loss: 2.5039, Valid Acc: 42.97%\n",
            "Epoch [301/500], Train Loss: 1.6701, Train Acc: 80.98%\n",
            "Epoch [302/500], Train Loss: 1.6421, Train Acc: 81.37%\n",
            "Epoch [303/500], Train Loss: 1.6248, Train Acc: 82.75%\n",
            "Epoch [304/500], Train Loss: 1.6687, Train Acc: 79.80%\n",
            "Epoch [305/500], Train Loss: 1.6468, Train Acc: 82.25%\n",
            "Epoch [306/500], Train Loss: 1.6043, Train Acc: 82.25%\n",
            "Epoch [307/500], Train Loss: 1.6698, Train Acc: 80.69%\n",
            "Epoch [308/500], Train Loss: 1.6571, Train Acc: 81.96%\n",
            "Epoch [309/500], Train Loss: 1.6279, Train Acc: 81.27%\n",
            "Epoch [310/500], Train Loss: 1.6173, Train Acc: 81.86%\n",
            "Epoch [311/500], Train Loss: 1.6287, Train Acc: 81.76%\n",
            "Epoch [312/500], Train Loss: 1.6017, Train Acc: 82.75%\n",
            "Epoch [313/500], Train Loss: 1.6445, Train Acc: 80.10%\n",
            "Epoch [314/500], Train Loss: 1.6586, Train Acc: 81.08%\n",
            "Epoch [315/500], Train Loss: 1.6169, Train Acc: 82.25%\n",
            "Epoch [316/500], Train Loss: 1.6533, Train Acc: 83.33%\n",
            "Epoch [317/500], Train Loss: 1.5744, Train Acc: 83.92%\n",
            "Epoch [318/500], Train Loss: 1.5935, Train Acc: 82.84%\n",
            "Epoch [319/500], Train Loss: 1.6226, Train Acc: 81.86%\n",
            "Epoch [320/500], Train Loss: 1.5750, Train Acc: 83.82%\n",
            "Epoch [321/500], Train Loss: 1.6138, Train Acc: 82.25%\n",
            "Epoch [322/500], Train Loss: 1.6281, Train Acc: 82.55%\n",
            "Epoch [323/500], Train Loss: 1.5459, Train Acc: 84.22%\n",
            "Epoch [324/500], Train Loss: 1.6010, Train Acc: 84.02%\n",
            "Epoch [325/500], Train Loss: 1.5874, Train Acc: 82.84%, Valid Loss: 2.4642, Valid Acc: 44.27%\n",
            "Epoch [326/500], Train Loss: 1.5889, Train Acc: 82.35%\n",
            "Epoch [327/500], Train Loss: 1.6050, Train Acc: 83.04%\n",
            "Epoch [328/500], Train Loss: 1.6244, Train Acc: 82.16%\n",
            "Epoch [329/500], Train Loss: 1.6104, Train Acc: 82.94%\n",
            "Epoch [330/500], Train Loss: 1.5861, Train Acc: 83.33%\n",
            "Epoch [331/500], Train Loss: 1.5675, Train Acc: 84.12%\n",
            "Epoch [332/500], Train Loss: 1.6184, Train Acc: 80.69%\n",
            "Epoch [333/500], Train Loss: 1.5462, Train Acc: 84.61%\n",
            "Epoch [334/500], Train Loss: 1.5843, Train Acc: 84.02%\n",
            "Epoch [335/500], Train Loss: 1.5717, Train Acc: 83.92%\n",
            "Epoch [336/500], Train Loss: 1.5517, Train Acc: 85.00%\n",
            "Epoch [337/500], Train Loss: 1.5731, Train Acc: 84.22%\n",
            "Epoch [338/500], Train Loss: 1.5761, Train Acc: 83.92%\n",
            "Epoch [339/500], Train Loss: 1.5791, Train Acc: 83.73%\n",
            "Epoch [340/500], Train Loss: 1.5985, Train Acc: 83.14%\n",
            "Epoch [341/500], Train Loss: 1.5491, Train Acc: 85.20%\n",
            "Epoch [342/500], Train Loss: 1.5490, Train Acc: 85.10%\n",
            "Epoch [343/500], Train Loss: 1.5350, Train Acc: 84.71%\n",
            "Epoch [344/500], Train Loss: 1.5661, Train Acc: 83.82%\n",
            "Epoch [345/500], Train Loss: 1.5436, Train Acc: 84.12%\n",
            "Epoch [346/500], Train Loss: 1.5387, Train Acc: 84.90%\n",
            "Epoch [347/500], Train Loss: 1.5713, Train Acc: 83.24%\n",
            "Epoch [348/500], Train Loss: 1.5540, Train Acc: 85.20%\n",
            "Epoch [349/500], Train Loss: 1.5878, Train Acc: 83.14%\n",
            "Epoch [350/500], Train Loss: 1.6065, Train Acc: 82.35%, Valid Loss: 2.4843, Valid Acc: 43.71%\n",
            "Epoch [351/500], Train Loss: 1.5553, Train Acc: 85.98%\n",
            "Epoch [352/500], Train Loss: 1.5544, Train Acc: 84.61%\n",
            "Epoch [353/500], Train Loss: 1.5622, Train Acc: 83.53%\n",
            "Epoch [354/500], Train Loss: 1.5383, Train Acc: 85.49%\n",
            "Epoch [355/500], Train Loss: 1.5153, Train Acc: 86.76%\n",
            "Epoch [356/500], Train Loss: 1.5431, Train Acc: 84.71%\n",
            "Epoch [357/500], Train Loss: 1.5556, Train Acc: 83.24%\n",
            "Epoch [358/500], Train Loss: 1.4981, Train Acc: 87.55%\n",
            "Epoch [359/500], Train Loss: 1.5265, Train Acc: 85.20%\n",
            "Epoch [360/500], Train Loss: 1.5198, Train Acc: 85.29%\n",
            "Epoch [361/500], Train Loss: 1.5325, Train Acc: 85.39%\n",
            "Epoch [362/500], Train Loss: 1.5528, Train Acc: 83.82%\n",
            "Epoch [363/500], Train Loss: 1.5268, Train Acc: 84.31%\n",
            "Epoch [364/500], Train Loss: 1.5820, Train Acc: 82.65%\n",
            "Epoch [365/500], Train Loss: 1.5650, Train Acc: 84.02%\n",
            "Epoch [366/500], Train Loss: 1.5240, Train Acc: 86.47%\n",
            "Epoch [367/500], Train Loss: 1.4965, Train Acc: 87.25%\n",
            "Epoch [368/500], Train Loss: 1.5147, Train Acc: 86.27%\n",
            "Epoch [369/500], Train Loss: 1.5145, Train Acc: 85.78%\n",
            "Epoch [370/500], Train Loss: 1.5232, Train Acc: 86.67%\n",
            "Epoch [371/500], Train Loss: 1.4869, Train Acc: 86.96%\n",
            "Epoch [372/500], Train Loss: 1.5069, Train Acc: 85.69%\n",
            "Epoch [373/500], Train Loss: 1.5541, Train Acc: 84.12%\n",
            "Epoch [374/500], Train Loss: 1.5356, Train Acc: 84.71%\n",
            "Epoch [375/500], Train Loss: 1.5087, Train Acc: 85.88%, Valid Loss: 2.4219, Valid Acc: 45.24%\n",
            "Epoch [376/500], Train Loss: 1.5062, Train Acc: 86.37%\n",
            "Epoch [377/500], Train Loss: 1.5040, Train Acc: 87.35%\n",
            "Epoch [378/500], Train Loss: 1.4981, Train Acc: 87.75%\n",
            "Epoch [379/500], Train Loss: 1.4867, Train Acc: 87.84%\n",
            "Epoch [380/500], Train Loss: 1.4909, Train Acc: 87.25%\n",
            "Epoch [381/500], Train Loss: 1.5357, Train Acc: 85.88%\n",
            "Epoch [382/500], Train Loss: 1.4831, Train Acc: 87.35%\n",
            "Epoch [383/500], Train Loss: 1.5390, Train Acc: 85.69%\n",
            "Epoch [384/500], Train Loss: 1.5414, Train Acc: 86.18%\n",
            "Epoch [385/500], Train Loss: 1.4694, Train Acc: 89.12%\n",
            "Epoch [386/500], Train Loss: 1.5080, Train Acc: 86.96%\n",
            "Epoch [387/500], Train Loss: 1.5153, Train Acc: 87.06%\n",
            "Epoch [388/500], Train Loss: 1.4869, Train Acc: 88.24%\n",
            "Epoch [389/500], Train Loss: 1.4674, Train Acc: 87.94%\n",
            "Epoch [390/500], Train Loss: 1.4643, Train Acc: 89.02%\n",
            "Epoch [391/500], Train Loss: 1.4923, Train Acc: 86.27%\n",
            "Epoch [392/500], Train Loss: 1.5170, Train Acc: 85.20%\n",
            "Epoch [393/500], Train Loss: 1.5042, Train Acc: 86.57%\n",
            "Epoch [394/500], Train Loss: 1.5222, Train Acc: 86.76%\n",
            "Epoch [395/500], Train Loss: 1.4722, Train Acc: 89.12%\n",
            "Epoch [396/500], Train Loss: 1.4843, Train Acc: 87.16%\n",
            "Epoch [397/500], Train Loss: 1.4795, Train Acc: 85.98%\n",
            "Epoch [398/500], Train Loss: 1.4774, Train Acc: 87.06%\n",
            "Epoch [399/500], Train Loss: 1.4825, Train Acc: 86.76%\n",
            "Epoch [400/500], Train Loss: 1.4824, Train Acc: 87.84%, Valid Loss: 2.4388, Valid Acc: 44.77%\n",
            "Epoch [401/500], Train Loss: 1.4645, Train Acc: 88.33%\n",
            "Epoch [402/500], Train Loss: 1.4893, Train Acc: 88.04%\n",
            "Epoch [403/500], Train Loss: 1.4439, Train Acc: 88.63%\n",
            "Epoch [404/500], Train Loss: 1.4455, Train Acc: 89.51%\n",
            "Epoch [405/500], Train Loss: 1.5242, Train Acc: 85.29%\n",
            "Epoch [406/500], Train Loss: 1.4470, Train Acc: 88.14%\n",
            "Epoch [407/500], Train Loss: 1.4748, Train Acc: 86.08%\n",
            "Epoch [408/500], Train Loss: 1.4706, Train Acc: 88.33%\n",
            "Epoch [409/500], Train Loss: 1.4919, Train Acc: 87.55%\n",
            "Epoch [410/500], Train Loss: 1.4688, Train Acc: 88.53%\n",
            "Epoch [411/500], Train Loss: 1.4541, Train Acc: 87.25%\n",
            "Epoch [412/500], Train Loss: 1.4689, Train Acc: 88.24%\n",
            "Epoch [413/500], Train Loss: 1.4750, Train Acc: 87.25%\n",
            "Epoch [414/500], Train Loss: 1.4889, Train Acc: 87.06%\n",
            "Epoch [415/500], Train Loss: 1.4651, Train Acc: 87.65%\n",
            "Epoch [416/500], Train Loss: 1.4515, Train Acc: 88.43%\n",
            "Epoch [417/500], Train Loss: 1.4826, Train Acc: 88.53%\n",
            "Epoch [418/500], Train Loss: 1.4541, Train Acc: 88.63%\n",
            "Epoch [419/500], Train Loss: 1.4546, Train Acc: 87.75%\n",
            "Epoch [420/500], Train Loss: 1.4333, Train Acc: 89.61%\n",
            "Epoch [421/500], Train Loss: 1.4451, Train Acc: 90.29%\n",
            "Epoch [422/500], Train Loss: 1.4487, Train Acc: 87.65%\n",
            "Epoch [423/500], Train Loss: 1.4606, Train Acc: 87.35%\n",
            "Epoch [424/500], Train Loss: 1.4390, Train Acc: 89.02%\n",
            "Epoch [425/500], Train Loss: 1.4317, Train Acc: 89.02%, Valid Loss: 2.3594, Valid Acc: 47.26%\n",
            "Epoch [426/500], Train Loss: 1.4580, Train Acc: 88.24%\n",
            "Epoch [427/500], Train Loss: 1.4553, Train Acc: 87.65%\n",
            "Epoch [428/500], Train Loss: 1.4123, Train Acc: 89.80%\n",
            "Epoch [429/500], Train Loss: 1.4526, Train Acc: 87.45%\n",
            "Epoch [430/500], Train Loss: 1.4719, Train Acc: 87.65%\n",
            "Epoch [431/500], Train Loss: 1.4539, Train Acc: 88.63%\n",
            "Epoch [432/500], Train Loss: 1.4443, Train Acc: 88.24%\n",
            "Epoch [433/500], Train Loss: 1.4306, Train Acc: 90.59%\n",
            "Epoch [434/500], Train Loss: 1.4555, Train Acc: 88.43%\n",
            "Epoch [435/500], Train Loss: 1.4321, Train Acc: 89.80%\n",
            "Epoch [436/500], Train Loss: 1.4402, Train Acc: 88.43%\n",
            "Epoch [437/500], Train Loss: 1.4442, Train Acc: 89.41%\n",
            "Epoch [438/500], Train Loss: 1.4459, Train Acc: 88.24%\n",
            "Epoch [439/500], Train Loss: 1.4369, Train Acc: 88.04%\n",
            "Epoch [440/500], Train Loss: 1.4218, Train Acc: 90.39%\n",
            "Epoch [441/500], Train Loss: 1.4309, Train Acc: 88.73%\n",
            "Epoch [442/500], Train Loss: 1.4262, Train Acc: 90.10%\n",
            "Epoch [443/500], Train Loss: 1.4337, Train Acc: 88.82%\n",
            "Epoch [444/500], Train Loss: 1.4362, Train Acc: 88.73%\n",
            "Epoch [445/500], Train Loss: 1.4349, Train Acc: 88.92%\n",
            "Epoch [446/500], Train Loss: 1.4253, Train Acc: 89.80%\n",
            "Epoch [447/500], Train Loss: 1.4507, Train Acc: 89.51%\n",
            "Epoch [448/500], Train Loss: 1.4516, Train Acc: 87.75%\n",
            "Epoch [449/500], Train Loss: 1.4292, Train Acc: 90.59%\n",
            "Epoch [450/500], Train Loss: 1.4341, Train Acc: 88.82%, Valid Loss: 2.3797, Valid Acc: 47.45%\n",
            "Epoch [451/500], Train Loss: 1.4118, Train Acc: 90.20%\n",
            "Epoch [452/500], Train Loss: 1.4318, Train Acc: 89.90%\n",
            "Epoch [453/500], Train Loss: 1.4213, Train Acc: 90.10%\n",
            "Epoch [454/500], Train Loss: 1.4484, Train Acc: 87.84%\n",
            "Epoch [455/500], Train Loss: 1.4299, Train Acc: 89.71%\n",
            "Epoch [456/500], Train Loss: 1.3910, Train Acc: 91.86%\n",
            "Epoch [457/500], Train Loss: 1.4059, Train Acc: 89.41%\n",
            "Epoch [458/500], Train Loss: 1.4162, Train Acc: 89.41%\n",
            "Epoch [459/500], Train Loss: 1.4113, Train Acc: 89.90%\n",
            "Epoch [460/500], Train Loss: 1.3996, Train Acc: 90.29%\n",
            "Epoch [461/500], Train Loss: 1.3996, Train Acc: 89.31%\n",
            "Epoch [462/500], Train Loss: 1.3944, Train Acc: 91.47%\n",
            "Epoch [463/500], Train Loss: 1.4348, Train Acc: 89.51%\n",
            "Epoch [464/500], Train Loss: 1.3835, Train Acc: 90.78%\n",
            "Epoch [465/500], Train Loss: 1.4085, Train Acc: 90.88%\n",
            "Epoch [466/500], Train Loss: 1.3904, Train Acc: 91.96%\n",
            "Epoch [467/500], Train Loss: 1.4105, Train Acc: 90.10%\n",
            "Epoch [468/500], Train Loss: 1.3991, Train Acc: 90.39%\n",
            "Epoch [469/500], Train Loss: 1.4148, Train Acc: 90.00%\n",
            "Epoch [470/500], Train Loss: 1.3912, Train Acc: 91.37%\n",
            "Epoch [471/500], Train Loss: 1.4243, Train Acc: 88.04%\n",
            "Epoch [472/500], Train Loss: 1.4024, Train Acc: 89.71%\n",
            "Epoch [473/500], Train Loss: 1.4049, Train Acc: 89.61%\n",
            "Epoch [474/500], Train Loss: 1.3770, Train Acc: 91.18%\n",
            "Epoch [475/500], Train Loss: 1.4159, Train Acc: 89.90%, Valid Loss: 2.3766, Valid Acc: 47.94%\n",
            "Epoch [476/500], Train Loss: 1.4083, Train Acc: 89.90%\n",
            "Epoch [477/500], Train Loss: 1.4127, Train Acc: 90.10%\n",
            "Epoch [478/500], Train Loss: 1.4079, Train Acc: 91.47%\n",
            "Epoch [479/500], Train Loss: 1.4055, Train Acc: 90.49%\n",
            "Epoch [480/500], Train Loss: 1.4084, Train Acc: 90.39%\n",
            "Epoch [481/500], Train Loss: 1.3961, Train Acc: 90.10%\n",
            "Epoch [482/500], Train Loss: 1.3979, Train Acc: 90.20%\n",
            "Epoch [483/500], Train Loss: 1.3769, Train Acc: 91.08%\n",
            "Epoch [484/500], Train Loss: 1.3532, Train Acc: 93.04%\n",
            "Epoch [485/500], Train Loss: 1.4035, Train Acc: 90.10%\n",
            "Epoch [486/500], Train Loss: 1.4172, Train Acc: 89.22%\n",
            "Epoch [487/500], Train Loss: 1.3757, Train Acc: 91.37%\n",
            "Epoch [488/500], Train Loss: 1.3755, Train Acc: 90.49%\n",
            "Epoch [489/500], Train Loss: 1.3673, Train Acc: 91.96%\n",
            "Epoch [490/500], Train Loss: 1.3940, Train Acc: 91.47%\n",
            "Epoch [491/500], Train Loss: 1.4069, Train Acc: 90.29%\n",
            "Epoch [492/500], Train Loss: 1.3839, Train Acc: 91.47%\n",
            "Epoch [493/500], Train Loss: 1.3583, Train Acc: 91.57%\n",
            "Epoch [494/500], Train Loss: 1.3875, Train Acc: 90.88%\n",
            "Epoch [495/500], Train Loss: 1.3830, Train Acc: 90.59%\n",
            "Epoch [496/500], Train Loss: 1.3940, Train Acc: 89.71%\n",
            "Epoch [497/500], Train Loss: 1.3724, Train Acc: 91.86%\n",
            "Epoch [498/500], Train Loss: 1.3809, Train Acc: 91.08%\n",
            "Epoch [499/500], Train Loss: 1.3928, Train Acc: 90.98%\n",
            "Epoch [500/500], Train Loss: 1.3699, Train Acc: 91.08%, Valid Loss: 2.3761, Valid Acc: 47.85%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 500\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Convolutional neural network\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=102):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(25088, 2048)\n",
        "        self.fc2 = nn.Linear(2048, num_classes)\n",
        "        self.drop = nn.Dropout(0.4)\n",
        "        # Batch normalisation used on convolution layers, dropout 20% used on fully connected linear layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.pool(out)\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.pool(out)\n",
        "        out = F.relu(self.bn3(self.conv3(out)))\n",
        "        out = self.pool(out)\n",
        "        out = F.relu(self.bn4(self.conv4(out)))\n",
        "        out = self.pool(out)\n",
        "        out = F.relu(self.bn5(self.conv5(out)))\n",
        "        out = self.pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.drop(self.fc1(out)))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Create model and push to device\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "# Get loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define transforms with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomVerticalFlip(p=0.05),\n",
        "    transforms.RandomHorizontalFlip(p=0.1),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.ColorJitter(contrast=0.25, saturation=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Flower102: Unknown number and various sized color images in 102 classes, with 40 to 258 images per class\n",
        "train_dataset = torchvision.datasets.Flowers102(root='./data', split='train',\n",
        "                                                download=True, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.Flowers102(root='./data', split='test',\n",
        "                                               download=True, transform=test_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device, epoch, smoothing=0.1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Apply label smoothing\n",
        "        num_classes = model.fc2.out_features\n",
        "        one_hot_labels = torch.zeros(labels.size(0), num_classes).to(device)\n",
        "        one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
        "        one_hot_labels = one_hot_labels * (1 - smoothing) + smoothing / num_classes\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, one_hot_labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    loss = train_loss / len(train_loader)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def test(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    loss = test_loss / len(test_loader)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "# Train the model\n",
        "train_losses, train_accs, valid_losses, valid_accs = [], [], [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    if ((epoch + 1) % 25 == 0) and (epoch >= 250):\n",
        "      valid_loss, valid_acc = test(model, test_loader, criterion, device)\n",
        "      valid_losses.append(valid_loss)   \n",
        "      valid_accs.append(valid_acc)\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.2f}%')\n",
        "    else:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Did not save model, so load model\n",
        "loaded_model = ConvNet()\n",
        "loaded_model.load_state_dict(torch.load(PATH)) # it takes the loaded dictionary, not the path file itself\n",
        "# Push to device\n",
        "loaded_model.to(device)\n",
        "loaded_model.eval()\n",
        "\n",
        "# Evaluation, no gradient required\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_correct2 = 0\n",
        "    n_samples = len(test_loader.dataset)\n",
        "\n",
        "    for images, labels in test_loader:  # Iterate over test loader\n",
        "        images = images.to(device)      # Push to GPU device\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)    # Compare outputs with lables\n",
        "        n_correct += (predicted == labels).sum().item() # Number of correct\n",
        "\n",
        "        outputs2 = loaded_model(images)       # Same put loaded model\n",
        "        _, predicted2 = torch.max(outputs2, 1)\n",
        "        n_correct2 += (predicted2 == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the model: {acc} %')\n",
        "\n",
        "    acc = 100.0 * n_correct2 / n_samples\n",
        "    print(f'Accuracy of the loaded model: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP8x83M5XMJH",
        "outputId": "7a83ee6c-924e-4cd0-f7f1-602f8834d746"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 47.845178077736215 %\n",
            "Accuracy of the loaded model: 47.845178077736215 %\n"
          ]
        }
      ]
    }
  ]
}